# federated-learning-CV-and-NLP
With the growth of data privacy needs, federated learning has become a key technology for collaborative training of models without sharing raw data. In fields such as computer vision and natural language processing, federated learning faces challenges such as degraded global model performance due to data heterogeneity (non-IID). This paper uses image classification and text classification tasks as carriers to study the fusion application of computer vision and natural language processing in federated learning. We select the CIFAR-100 image dataset and ResNet-110 convolutional neural network as the main computer vision experiment to compare and analyze the effects of the classical federated average algorithm on the accuracy and convergence of the global model with two improved strategies, the attention aggregation mechanism based on the validation set and the algorithm that introduces contrast loss. At the same time, the AG News news text dataset and ALBERT pre-training model are used to conduct federated learning experiments in natural language processing tasks to verify the effectiveness of the above strategies in text scenarios. Experimental results show that the benchmark performance of FedAvg is significantly lower than that of centralized training under non-independent and homogeneous data division, while the attention mechanism and contrastive learning strategy improve the accuracy and convergence speed of the global model to a certain extent. Among them, the aggregation method with attention weighting can dynamically adjust the client weight and improve the adaptability of the global model to different client data distribution. The comparison loss effectively alleviates the representation bias caused by data heterogeneity and significantly improves the generalization performance of the model. Assisted NLP experiments also show a similar trend.
